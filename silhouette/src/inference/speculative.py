"""
NANOSILHOUETTE - Speculative Decoding
======================================
Implements draft-then-verify decoding for 2-3x faster inference.
Based on research from Google, DeepMind, and IBM (2024).
"""
import torch
import torch.nn as nn
from typing import Optional, Tuple, List
from dataclasses import dataclass


@dataclass
class SpeculativeConfig:
    """Configuration for speculative decoding."""
    num_speculative_tokens: int = 4  # Number of tokens to speculate
    temperature: float = 0.7
    top_k: int = 50
    top_p: float = 0.9
    max_new_tokens: int = 100


class SpeculativeDecoder:
    """
    Speculative Decoding for faster inference.
    
    Uses a smaller draft model to propose tokens,
    then verifies with the main model in parallel.
    """
    
    def __init__(
        self,
        target_model: nn.Module,
        draft_model: Optional[nn.Module] = None,
        config: Optional[SpeculativeConfig] = None
    ):
        self.target_model = target_model
        self.draft_model = draft_model
        self.config = config or SpeculativeConfig()
        
        # If no draft model provided, use self-drafting
        # (target model with early exit)
        self.use_self_drafting = draft_model is None
    
    @torch.no_grad()
    def generate(
        self,
        input_ids: torch.Tensor,
        max_new_tokens: Optional[int] = None
    ) -> torch.Tensor:
        """
        Generate tokens using speculative decoding.
        
        Args:
            input_ids: (batch, seq) input token IDs
            max_new_tokens: Maximum tokens to generate
        
        Returns:
            Generated token IDs
        """
        max_new_tokens = max_new_tokens or self.config.max_new_tokens
        device = input_ids.device
        batch_size = input_ids.shape[0]
        
        generated_tokens = 0
        current_ids = input_ids.clone()
        
        while generated_tokens < max_new_tokens:
            # 1. Draft phase: generate speculative tokens
            draft_tokens = self._draft_tokens(current_ids)
            
            # 2. Verify phase: check with target model
            accepted_tokens = self._verify_tokens(current_ids, draft_tokens)
            
            # 3. Accept verified tokens
            if len(accepted_tokens) > 0:
                accepted_tensor = torch.tensor(
                    accepted_tokens, device=device
                ).unsqueeze(0).expand(batch_size, -1)
                current_ids = torch.cat([current_ids, accepted_tensor], dim=1)
                generated_tokens += len(accepted_tokens)
            else:
                # If no tokens accepted, sample one from target
                next_token = self._sample_target(current_ids)
                current_ids = torch.cat([current_ids, next_token], dim=1)
                generated_tokens += 1
        
        return current_ids
    
    def _draft_tokens(
        self,
        input_ids: torch.Tensor
    ) -> List[int]:
        """Generate speculative tokens using draft model."""
        draft_ids = input_ids.clone()
        draft_tokens = []
        
        draft_model = self.draft_model or self.target_model
        
        for _ in range(self.config.num_speculative_tokens):
            outputs = draft_model(draft_ids)
            logits = outputs["logits"][:, -1, :] / self.config.temperature
            
            # Apply top-k
            if self.config.top_k > 0:
                values, _ = logits.topk(self.config.top_k)
                logits[logits < values[:, [-1]]] = float('-inf')
            
            # Sample
            probs = torch.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            
            draft_tokens.append(next_token.item())
            draft_ids = torch.cat([draft_ids, next_token], dim=1)
        
        return draft_tokens
    
    def _verify_tokens(
        self,
        input_ids: torch.Tensor,
        draft_tokens: List[int]
    ) -> List[int]:
        """Verify draft tokens with target model."""
        if not draft_tokens:
            return []
        
        # Build sequence with all draft tokens
        device = input_ids.device
        all_tokens = input_ids.clone()
        
        for token in draft_tokens:
            token_tensor = torch.tensor([[token]], device=device)
            all_tokens = torch.cat([all_tokens, token_tensor], dim=1)
        
        # Get target model predictions for all positions
        outputs = self.target_model(all_tokens)
        logits = outputs["logits"]
        
        # Verify each draft token
        accepted = []
        input_len = input_ids.shape[1]
        
        for i, draft_token in enumerate(draft_tokens):
            pos = input_len + i - 1
            if pos < 0:
                pos = 0
            
            # Get target distribution at this position
            target_logits = logits[:, pos, :] / self.config.temperature
            target_probs = torch.softmax(target_logits, dim=-1)
            
            # Acceptance probability
            draft_prob = target_probs[0, draft_token].item()
            
            # Accept with probability based on target model's confidence
            if torch.rand(1).item() < draft_prob * 2:  # Relaxed acceptance
                accepted.append(draft_token)
            else:
                break  # Reject rest of sequence
        
        return accepted
    
    def _sample_target(self, input_ids: torch.Tensor) -> torch.Tensor:
        """Sample a single token from target model."""
        outputs = self.target_model(input_ids)
        logits = outputs["logits"][:, -1, :] / self.config.temperature
        
        # Apply top-k
        if self.config.top_k > 0:
            values, _ = logits.topk(self.config.top_k)
            logits[logits < values[:, [-1]]] = float('-inf')
        
        probs = torch.softmax(logits, dim=-1)
        return torch.multinomial(probs, num_samples=1)


class MedusaHead(nn.Module):
    """
    Medusa-style multi-token prediction head.
    
    Predicts multiple future tokens in parallel.
    """
    
    def __init__(
        self,
        d_model: int,
        vocab_size: int,
        num_heads: int = 4,  # Number of prediction heads
        hidden_dim: Optional[int] = None
    ):
        super().__init__()
        self.num_heads = num_heads
        hidden_dim = hidden_dim or d_model
        
        # Each head predicts one future token
        self.heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d_model, hidden_dim),
                nn.SiLU(),
                nn.Linear(hidden_dim, vocab_size)
            )
            for _ in range(num_heads)
        ])
    
    def forward(
        self,
        hidden_states: torch.Tensor
    ) -> List[torch.Tensor]:
        """
        Predict multiple future tokens.
        
        Args:
            hidden_states: (batch, seq, d_model)
        
        Returns:
            List of logits for each future position
        """
        # Get last hidden state
        last_hidden = hidden_states[:, -1, :]  # (batch, d_model)
        
        # Predict from each head
        predictions = [head(last_hidden) for head in self.heads]
        
        return predictions
    
    def speculative_tokens(
        self,
        hidden_states: torch.Tensor,
        temperature: float = 1.0
    ) -> torch.Tensor:
        """Get speculative token predictions."""
        predictions = self.forward(hidden_states)
        
        tokens = []
        for logits in predictions:
            probs = torch.softmax(logits / temperature, dim=-1)
            token = torch.argmax(probs, dim=-1)
            tokens.append(token)
        
        return torch.stack(tokens, dim=1)  # (batch, num_heads)


def create_speculative_decoder(
    target_model: nn.Module,
    draft_model: Optional[nn.Module] = None,
    num_speculative_tokens: int = 4
) -> SpeculativeDecoder:
    """Factory function for speculative decoder."""
    config = SpeculativeConfig(num_speculative_tokens=num_speculative_tokens)
    return SpeculativeDecoder(target_model, draft_model, config)


if __name__ == "__main__":
    print("Testing Speculative Decoding...")
    
    # Mock model for testing
    class MockModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.embed = nn.Embedding(1000, 256)
            self.fc = nn.Linear(256, 1000)
        
        def forward(self, x):
            h = self.embed(x).mean(dim=1, keepdim=True).expand(-1, x.shape[1], -1)
            return {"logits": self.fc(h)}
    
    model = MockModel()
    decoder = create_speculative_decoder(model, num_speculative_tokens=3)
    
    input_ids = torch.randint(0, 1000, (1, 10))
    output = decoder.generate(input_ids, max_new_tokens=10)
    print(f"Input: {input_ids.shape}")
    print(f"Output: {output.shape}")
    
    # Test Medusa head
    medusa = MedusaHead(256, 1000, num_heads=4)
    hidden = torch.randn(1, 10, 256)
    preds = medusa(hidden)
    print(f"Medusa predictions: {len(preds)} heads")
    
    print("âœ… Speculative Decoding test passed!")
