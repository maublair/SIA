"""
SILHOUETTE - Main Model
=======================
Hybrid architecture combining Mamba SSM + Transformer + CMS + JEPA + Introspection.
Homeostatic auto-scaling based on detected hardware environment.

NEVER disables AGI capabilities - only scales them proportionally.
"""
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Dict, List, Tuple, Any
from dataclasses import dataclass, field

from .mamba_block import MambaBlock, MambaConfig
from .transformer_block import TransformerBlock, TransformerConfig
from .cms_memory import ContinuumMemorySystem, CMSConfig
from .jepa_head import JEPAHead, JEPAConfig
from .introspection import IntrospectionModule, IntrospectionConfig
from .moe import MoELayer, MoEConfig
from .deep_optimizer import DeepOptimizer, DeepOptimizerConfig
from .homeostasis import get_homeostasis_manager, get_optimal_config, ScalableConfig


@dataclass
class SilhouetteConfig:
    """
    Model configuration with homeostatic auto-scaling support.
    
    This config can be auto-generated by the HomeostasisManager
    to optimally utilize available hardware resources.
    """
    # Core dimensions
    vocab_size: int = 32000
    d_model: int = 512
    intermediate_size: int = 1376
    num_layers: int = 12
    num_heads: int = 8
    num_kv_heads: int = 2
    max_seq_len: int = 2048
    
    # Hybrid ratio (Mamba : Transformer)
    mamba_ratio: int = 7  # 7 Mamba blocks per 1 Transformer
    
    # Component toggles (ALWAYS True in homeostatic mode - scaled, not disabled)
    use_mamba: bool = True
    use_transformer: bool = True
    use_cms: bool = True
    use_jepa: bool = True
    use_introspection: bool = True
    use_moe: bool = True  # Mixture of Experts (Jamba style)
    use_deep_optimizer: bool = True  # Hope architecture
    
    # MoE settings
    num_experts: int = 16
    num_experts_per_tok: int = 2
    moe_interval: int = 2  # Apply MoE every N layers
    
    # CMS settings
    cms_timescales: Tuple[float, ...] = (1.0, 10.0, 100.0, 1000.0)
    cms_memory_size: int = 256
    
    # AGI Core scaling (0.0 - 1.0)
    agi_core_scale: float = 1.0
    
    # Normalization
    rms_norm_eps: float = 1e-5
    tie_embeddings: bool = True
    
    # Quantization and optimization
    quantization: str = "none"  # "none", "fp16", "8bit", "4bit"
    gradient_checkpointing: bool = False
    
    # ============================================
    # HOMEOSTATIC AUTO-CONFIGURATION
    # ============================================
    
    @classmethod
    def from_homeostasis(cls, force_profile: Optional[str] = None) -> "SilhouetteConfig":
        """
        Create config using the Homeostatic Resource Manager.
        
        This is the recommended way to create a config as it:
        - Auto-detects available resources (GPU, RAM, CPU)
        - Considers other running GPU processes
        - NEVER disables AGI capabilities, only scales them
        - Applies optimal quantization settings
        
        Args:
            force_profile: Optional profile name to force
                          ('cpu_only', 'ultra_constrained', 'constrained', 
                           'balanced', 'performance', 'unlimited')
        
        Returns:
            Optimally configured SilhouetteConfig
        """
        scalable = get_optimal_config(force_profile)
        
        # Convert ScalableConfig to SilhouetteConfig
        return cls(
            vocab_size=scalable.vocab_size,
            d_model=scalable.d_model,
            intermediate_size=scalable.intermediate_size,
            num_layers=scalable.num_layers,
            num_heads=scalable.num_heads,
            num_kv_heads=scalable.num_kv_heads,
            max_seq_len=scalable.max_seq_len,
            mamba_ratio=scalable.mamba_ratio,
            use_moe=scalable.use_moe,
            num_experts=scalable.num_experts,
            num_experts_per_tok=scalable.num_experts_per_tok,
            moe_interval=scalable.moe_interval,
            use_cms=scalable.use_cms,
            cms_memory_size=scalable.cms_memory_size,
            cms_timescales=tuple([1.0, 10.0, 100.0, 1000.0][:scalable.cms_num_timescales]),
            use_jepa=scalable.use_jepa,
            use_introspection=scalable.use_introspection,
            use_deep_optimizer=scalable.use_deep_optimizer,
            agi_core_scale=scalable.agi_core_scale,
            quantization=scalable.quantization,
            gradient_checkpointing=scalable.gradient_checkpointing,
        )
    
    @classmethod
    def auto_detect(cls) -> "SilhouetteConfig":
        """
        DEPRECATED: Use from_homeostasis() instead.
        
        Legacy auto-detect for backwards compatibility.
        Now delegates to the homeostatic system.
        """
        print("[SILHOUETTE] Note: auto_detect() is deprecated. Use from_homeostasis() for better resource optimization.")
        return cls.from_homeostasis()
    
    # Legacy presets (maintained for compatibility)
    @classmethod
    def nano(cls) -> "SilhouetteConfig":
        """Legacy: Use from_homeostasis('constrained') instead."""
        return cls.from_homeostasis('constrained')
    
    @classmethod
    def micro(cls) -> "SilhouetteConfig":
        """Legacy: Use from_homeostasis('balanced') instead."""
        return cls.from_homeostasis('balanced')
    
    @classmethod
    def small(cls) -> "SilhouetteConfig":
        """Legacy: Use from_homeostasis('performance') instead."""
        return cls.from_homeostasis('performance')
    
    @classmethod
    def medium(cls) -> "SilhouetteConfig":
        """Legacy: Use from_homeostasis('unlimited') instead."""
        return cls.from_homeostasis('unlimited')


class RMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-5):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))
    
    def forward(self, x):
        rms = torch.sqrt(x.float().pow(2).mean(-1, keepdim=True) + self.eps)
        return (self.weight * (x.float() / rms)).to(x.dtype)


class HybridBlock(nn.Module):
    """Single hybrid block: either Mamba or Transformer based on position."""
    def __init__(self, config: SilhouetteConfig, block_idx: int):
        super().__init__()
        self.block_idx = block_idx
        self.use_transformer = (block_idx + 1) % (config.mamba_ratio + 1) == 0
        
        if self.use_transformer and config.use_transformer:
            self.block = TransformerBlock(TransformerConfig(
                d_model=config.d_model,
                num_heads=config.num_heads,
                num_kv_heads=config.num_kv_heads,
                intermediate_size=config.intermediate_size,
                rms_norm_eps=config.rms_norm_eps
            ))
            self.block_type = "transformer"
        else:
            self.block = MambaBlock(MambaConfig(
                d_model=config.d_model,
                d_state=16,
                d_conv=4,
                expand=2
            ))
            self.block_type = "mamba"
    
    def forward(self, x, **kwargs):
        if self.block_type == "transformer":
            return self.block(x, **kwargs)
        else:
            out, cache = self.block(x)
            return out, cache


class SilhouetteModel(nn.Module):
    """
    SILHOUETTE: Homeostatic AGI Language Model
    
    Architecture:
    - Hybrid Mamba/Transformer backbone (7:1 ratio)
    - Continuum Memory System (multi-timescale)
    - JEPA embedding predictor
    - Introspection module
    - Homeostatic auto-scaling based on hardware
    
    All AGI capabilities are ALWAYS present, scaled proportionally
    to available resources. Nothing is ever disabled.
    """
    
    def __init__(self, config: Optional[SilhouetteConfig] = None):
        super().__init__()
        self.config = config or SilhouetteConfig.from_homeostasis()
        
        # Token embeddings
        self.embed_tokens = nn.Embedding(self.config.vocab_size, self.config.d_model)
        
        # Hybrid blocks
        self.layers = nn.ModuleList([
            HybridBlock(self.config, i) for i in range(self.config.num_layers)
        ])
        
        # CMS (between layers at intervals)
        if self.config.use_cms:
            self.cms = ContinuumMemorySystem(CMSConfig(
                d_model=self.config.d_model,
                num_timescales=len(self.config.cms_timescales),
                timescale_factors=self.config.cms_timescales,
                memory_size=self.config.cms_memory_size
            ))
            self.cms_interval = max(1, self.config.num_layers // 3)
        
        # Final norm
        self.norm = RMSNorm(self.config.d_model, self.config.rms_norm_eps)
        
        # LM head
        self.lm_head = nn.Linear(self.config.d_model, self.config.vocab_size, bias=False)
        if self.config.tie_embeddings:
            self.lm_head.weight = self.embed_tokens.weight
        
        # JEPA head
        if self.config.use_jepa:
            self.jepa = JEPAHead(JEPAConfig(
                d_model=self.config.d_model,
                embedding_dim=self.config.d_model
            ))
        
        # Introspection
        if self.config.use_introspection:
            self.introspection = IntrospectionModule(IntrospectionConfig(
                d_model=self.config.d_model
            ))
        
        # MoE layers (Jamba style: every N layers)
        if self.config.use_moe:
            self.moe_layers = nn.ModuleDict()
            for i in range(self.config.num_layers):
                if (i + 1) % self.config.moe_interval == 0:
                    self.moe_layers[str(i)] = MoELayer(MoEConfig(
                        d_model=self.config.d_model,
                        intermediate_size=self.config.intermediate_size,
                        num_experts=self.config.num_experts,
                        num_experts_per_tok=self.config.num_experts_per_tok
                    ))
        
        # Deep Optimizer (Hope architecture)
        if self.config.use_deep_optimizer:
            self.deep_optimizer = DeepOptimizer(DeepOptimizerConfig(
                d_model=self.config.d_model,
                optimizer_hidden_dim=min(256, self.config.d_model // 2),
                num_optimizer_steps=3
            ))
        
        self.apply(self._init_weights)
        
        # Log initialization with profile info
        profile_name = getattr(self.config, 'quantization', 'unknown')
        print(f"[SILHOUETTE] Initialized with {self.num_parameters():,} parameters")
        print(f"[SILHOUETTE] Config: d_model={self.config.d_model}, layers={self.config.num_layers}, AGI scale={getattr(self.config, 'agi_core_scale', 1.0)*100:.0f}%")
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def num_parameters(self, trainable_only: bool = True) -> int:
        return sum(p.numel() for p in self.parameters() if not trainable_only or p.requires_grad)
    
    def forward(
        self,
        input_ids: torch.Tensor,
        labels: Optional[torch.Tensor] = None,
        return_introspection: bool = False
    ) -> Dict[str, torch.Tensor]:
        """
        Forward pass.
        
        Args:
            input_ids: (batch, seq_len) token IDs
            labels: Optional (batch, seq_len) for training
            return_introspection: Whether to return introspection data
        
        Returns:
            Dict with logits, loss, and optional introspection data
        """
        h = self.embed_tokens(input_ids)
        
        # CMS state
        cms_states = None
        
        # Auxiliary losses from MoE
        moe_aux_loss = 0.0
        
        # Process through hybrid layers
        for i, layer in enumerate(self.layers):
            h, _ = layer(h)
            
            # Apply MoE at intervals (Jamba style)
            if self.config.use_moe and str(i) in self.moe_layers:
                h_moe, aux_loss = self.moe_layers[str(i)](h)
                h = h + h_moe  # Residual connection
                moe_aux_loss = moe_aux_loss + aux_loss
            
            # Apply CMS at intervals
            if self.config.use_cms and (i + 1) % self.cms_interval == 0:
                h, cms_states = self.cms(h, cms_states)
        
        # Apply Deep Optimizer (Hope architecture)
        deep_opt_info = None
        if self.config.use_deep_optimizer:
            h, deep_opt_info = self.deep_optimizer(h)
        
        h = self.norm(h)
        
        # LM logits
        logits = self.lm_head(h)
        
        # Compute losses
        outputs = {"logits": logits, "hidden_states": h}
        total_loss = None
        
        if labels is not None:
            # Cross-entropy loss
            ce_loss = F.cross_entropy(
                logits.view(-1, self.config.vocab_size),
                labels.view(-1),
                ignore_index=-100
            )
            outputs["ce_loss"] = ce_loss
            total_loss = ce_loss
            
            # JEPA loss
            if self.config.use_jepa:
                _, jepa_loss = self.jepa(h, h)
                if jepa_loss is not None:
                    outputs["jepa_loss"] = jepa_loss
                    total_loss = total_loss + 0.1 * jepa_loss
            
            # MoE auxiliary loss (load balancing)
            if self.config.use_moe and moe_aux_loss != 0.0:
                if isinstance(moe_aux_loss, torch.Tensor):
                    outputs["moe_aux_loss"] = moe_aux_loss
                    total_loss = total_loss + moe_aux_loss
            
            outputs["loss"] = total_loss
        
        # Introspection
        if return_introspection and self.config.use_introspection:
            outputs["introspection"] = self.introspection(h)
        
        return outputs
    
    @torch.no_grad()
    def generate(
        self,
        input_ids: torch.Tensor,
        max_new_tokens: int = 100,
        temperature: float = 0.7,
        top_p: float = 0.9,
        top_k: int = 50
    ) -> torch.Tensor:
        """Simple autoregressive generation."""
        for _ in range(max_new_tokens):
            # Truncate if too long
            idx_cond = input_ids[:, -self.config.max_seq_len:]
            
            # Forward
            outputs = self(idx_cond)
            logits = outputs["logits"][:, -1, :] / temperature
            
            # Top-k filtering
            if top_k > 0:
                indices_to_remove = logits < logits.topk(top_k)[0][..., -1, None]
                logits[indices_to_remove] = float('-inf')
            
            # Top-p (nucleus) filtering
            if top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                sorted_mask = cumulative_probs > top_p
                sorted_mask[..., 1:] = sorted_mask[..., :-1].clone()
                sorted_mask[..., 0] = False
                indices_to_remove = sorted_mask.scatter(1, sorted_indices, sorted_mask)
                logits[indices_to_remove] = float('-inf')
            
            # Sample
            probs = F.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            input_ids = torch.cat([input_ids, next_token], dim=1)
        
        return input_ids


def create_model(variant: str = "auto") -> SilhouetteModel:
    """
    Factory function to create SILHOUETTE model.
    
    Args:
        variant: Profile to use. Options:
            - "auto": Auto-detect based on hardware (recommended)
            - "cpu_only": CPU mode with minimal resources
            - "constrained": RTX 3050 4GB (formerly "nano")
            - "balanced": RTX 3060/3070 8GB (formerly "micro")
            - "performance": RTX 3080+ 12-16GB (formerly "small")
            - "unlimited": RTX 4090 / A100 24GB+ (formerly "medium")
    
    Returns:
        Configured SilhouetteModel instance
    """
    # Map legacy names to new profiles
    legacy_map = {
        "nano": "constrained",
        "micro": "balanced",
        "small": "performance",
        "medium": "unlimited",
        "auto": None  # None triggers auto-detection
    }
    
    profile = legacy_map.get(variant, variant)
    config = SilhouetteConfig.from_homeostasis(profile)
    return SilhouetteModel(config)


# Backward compatibility aliases
NanoSilhouetteConfig = SilhouetteConfig
NanoSilhouetteModel = SilhouetteModel


if __name__ == "__main__":
    print("Testing SILHOUETTE Model with Homeostatic Resource Management...")
    print("=" * 60)
    
    # Test with auto-detection
    model = create_model("auto")
    
    # Test forward
    x = torch.randint(0, 32000, (2, 64))
    outputs = model(x, labels=x, return_introspection=True)
    
    print(f"\nModel test results:")
    print(f"  Logits shape: {outputs['logits'].shape}")
    print(f"  Loss: {outputs['loss'].item():.4f}")
    if "introspection" in outputs:
        print(f"  Introspection anomaly: {outputs['introspection']['anomaly_score'].mean():.4f}")
    
    # Test generation
    gen = model.generate(x[:1, :10], max_new_tokens=20)
    print(f"  Generated shape: {gen.shape}")
    
    print("\nâœ… SILHOUETTE homeostatic model test passed!")
