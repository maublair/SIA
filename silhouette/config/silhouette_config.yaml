# ============================================
# SILHOUETTE - Nano Variant Configuration
# ============================================
# 50M parameters, optimized for RTX 3050 (4GB VRAM)
# Independent instance for Silhouette Agency OS

model:
  name: "silhouette-nano"
  variant: "nano"
  
  # Architecture Dimensions
  vocab_size: 32000
  hidden_size: 512
  intermediate_size: 1376  # ~2.7x hidden for SwiGLU
  num_layers: 12
  num_attention_heads: 8
  num_key_value_heads: 2   # GQA: 4 queries per KV head
  max_position_embeddings: 2048
  
  # Hybrid Architecture (Jamba-style)
  mamba_to_transformer_ratio: 7  # 7 Mamba blocks per 1 Transformer
  use_mamba: true
  use_transformer: true
  
  # Mamba SSM Configuration
  mamba:
    d_state: 16          # State dimension
    d_conv: 4            # Conv kernel size
    expand: 2            # Expansion factor
    dt_rank: "auto"      # Delta rank (auto = hidden_size / 16)
    dt_min: 0.001
    dt_max: 0.1
    dt_init: "random"
    dt_scale: 1.0
    bias: false
    conv_bias: true
  
  # Transformer Configuration
  transformer:
    attention_type: "gqa"  # Grouped Query Attention
    use_flash_attention: true
    attention_dropout: 0.0
    use_rotary_embeddings: true
    rotary_dim: 64  # hidden_size / num_attention_heads
    rotary_base: 10000
  
  # Continuum Memory System (Nested Learning)
  cms:
    enabled: true
    num_timescales: 4
    timescale_factors: [1.0, 10.0, 100.0, 1000.0]  # τ_fast to τ_slow
    memory_size: 256
  
  # JEPA Embedding Predictor
  jepa:
    enabled: true
    embedding_dim: 512
    num_predictor_layers: 2
    use_infonce_loss: true
    temperature: 0.07
  
  # Introspection Module
  introspection:
    enabled: true
    num_features: 128
    monitor_interval: 100  # steps between state reports
  
  # Normalization & Activations
  rms_norm_eps: 1e-5
  hidden_act: "silu"     # SiLU/Swish activation
  tie_word_embeddings: true

training:
  # Optimization
  learning_rate: 3e-4
  weight_decay: 0.1
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  
  # LR Schedule
  lr_scheduler_type: "cosine"
  warmup_steps: 1000
  num_training_steps: 100000
  
  # Batch Size
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 8
  effective_batch_size: 32  # 4 * 8
  
  # Precision
  mixed_precision: "bf16"  # or "fp16" for older GPUs
  gradient_checkpointing: true
  
  # Loss Weights
  loss_weights:
    cross_entropy: 1.0
    infonce: 0.1
    introspection: 0.01
  
  # Logging
  logging_steps: 10
  eval_steps: 500
  save_steps: 1000
  
  # Data
  max_seq_length: 1024
  dataloader_num_workers: 4

inference:
  # Generation
  max_new_tokens: 256
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  do_sample: true
  
  # Quantization for deployment
  quantization:
    enabled: false
    bits: 4
    method: "nf4"  # or "gptq", "gguf"
  
  # Batching
  batch_size: 1
  use_cache: true

hardware:
  # Target: NVIDIA RTX 3050 4GB
  device: "cuda"
  device_ids: [0]
  max_memory_gb: 3.5  # Leave 0.5GB headroom
  compile_model: true  # torch.compile for speedup
